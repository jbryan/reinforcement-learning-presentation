% vim:tw=70
\documentclass[ignorenonframetext]{beamer}
%\documentclass{article}
%\usepackage{beamerarticle}

\mode<presentation>{\usetheme{Darmstadt}}
%\usetheme{default}
%\usetheme{Singapore}
%\usetheme{Dresden}
%\usetheme{Luebeck}
%\usetheme{Antibes}
%\usetheme{Malmoe}

\mode<presentation>{\usepackage{latex/elephantbird}}
%\usecolortheme{beetle}
%\usecolortheme{seahorse}
%\usecolortheme{dolphin}
%\usecolortheme{whale}
%\usecolortheme{fly}

\usepackage{rotating}
\usepackage[utf8]{inputenc}

\usefonttheme{professionalfonts}
%\usefonttheme{serif}

\def\nq{\hspace{-1em}}
\def\look{\(\uparrow\)}
\def\ignore#1{}
\def\deltabar{{\delta\!\!\!^{-}}}
\def\qed{\sqcap\!\!\!\!\sqcup}
\def\odt{{\textstyle{1\over 2}}}
\def\odf{{\textstyle{1\over 4}}}
\def\odA{{\textstyle{1\over A}}}
\def\hbar{h\!\!\!\!^{-}\,}
\def\dbar{d\!\!^{-}\!}
\def\eps{\varepsilon}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\beqn{\begin{displaymath}}
\def\eeqn{\end{displaymath}}
\def\bqa{\begin{equation}\begin{array}{c}}
\def\eqa{\end{array}\end{equation}}
\def\bqan{\begin{displaymath}\begin{array}{c}}
\def\eqan{\end{array}\end{displaymath}}
\def\pb{\underline}                       % probability notation
\def\pb#1{\underline{#1}}                 % probability notation
\def\blank{{\,_\sqcup\,}}                 % blank position
\def\maxarg{\mathop{\rm maxarg}}          % maxarg
\def\minarg{\mathop{\rm minarg}}          % minarg
\def\hh#1{{\dot{#1}}}                     % historic I/O
\def\best{*}                              % or {best}
\def\vec#1{{\bf #1}}
\def\length{{l}}

\title{Reinforcement Learning}
\author{Josh Bryan}
\institute{University of Illinois at Chicago, MCS 548}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
\mode<presentation>{
\subsection{Title}
\begin{frame}
	\titlepage
\end{frame}
\subsection{Contents}
\begin{frame}[allowframebreaks]
	\tableofcontents
\end{frame}
}


\subsection{Reinforcement Learning Overview}

	Reinforcement Learning is an area of AI at the boundary between machine
	learning and decision theory that also incorporates ideas from
	psychology.  The basic learning task is to learn an agent function,
	a function mapping a sequence of observations to an action, by
	actively participating in and exploring an environment.  The
	fundamental idea is that for every action the agent receives some form of
	reward signal.  That signal is the agents indication of success, and
	its task is to maximize the expectation of average or discounted sum
	of rewards.  For the most part this presentation will follow the
	the exposition given in Chapter 13 of \cite{mitchell_machine_1997},
	however ideas will be borrowed from
	\cite{kaelbling_reinforcement_1996} and
	\cite{russell_artificial_2010} as well.  

\begin{frame}
	\frametitle{What is Reinforcement Learning?}
	\begin{block}{}
		Reinforcement learning takes psychology, decision theory, and
		learning theory to answery ``How should an agent learn to act?''
	\end{block}
	\begin{itemize}
		\item Classical Conditioning (see Pavlov's Dog)
		\item Utility Theory
		\item (Partially Observable) Markov Decision Processes
		\item Learns an ``Agent Function''
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Agent Function}
	\begin{itemize}
		\item $O$ is a set of observation symbols.
		\item $A$ is a set of actions.
	\end{itemize}
	\begin{block}{}
		\begin{center}
			$f: O^* \rightarrow A $
		\end{center}
	\end{block}
\end{frame}


\section{Markov Decision Processes}

\subsection{Model Introduction}

\begin{frame}
	\frametitle{What is an MDP?}
\end{frame}

\begin{frame}
	\frametitle{Simple Example}
\end{frame}

\subsection{Policy}

\begin{frame}
	\frametitle{Policy Defined}
\end{frame}

\begin{frame}
	\frametitle{Value of Policy}
\end{frame}

\begin{frame}
	\frametitle{Bellman Equations}
\end{frame}

\subsection{Learning Task}

\begin{frame}
	\frametitle{What are we learning?}
\end{frame}

\begin{frame}
	\frametitle{What are we given?}
\end{frame}

\begin{frame}
	\frametitle{Model Free vs. Model Lased Learning}
\end{frame}

\section{Q-Learning}

\begin{frame}
\end{frame}


\section{SARSA-$\lambda$}


\section{Conclusions}
\subsection{Sources}
\begin{frame}[allowframebreaks]
	\frametitle{Sources}
	\nocite{*}
	\bibliography{reinforcement_learning}
	\bibliographystyle{abbrv}
\end{frame}
\end{document}


% vim:tw=70
\documentclass[ignorenonframetext]{beamer}
%\documentclass{article}
%\usepackage{beamerarticle}

\mode<presentation>{\usetheme{Darmstadt}}
%\usetheme{default}
%\usetheme{Singapore}
%\usetheme{Dresden}
%\usetheme{Luebeck}
%\usetheme{Antibes}
%\usetheme{Malmoe}

\mode<presentation>{\usepackage{latex/elephantbird}}
%\usecolortheme{beetle}
%\usecolortheme{seahorse}
%\usecolortheme{dolphin}
%\usecolortheme{whale}
%\usecolortheme{fly}

\usepackage{rotating}
\usepackage[utf8]{inputenc}

\usepackage{amsmath,amssymb,amsfonts,textcomp}
\usepackage{amsthm}
\usepackage{array}
\usepackage[english]{babel}
\usepackage[T1]{fontenc}
\usepackage{graphicx}
%\usepackage{theorem}
\usepackage{enumerate}
\usepackage{color}
\usepackage{framed}
\usepackage{url}
\usepackage{hyperref}
\usefonttheme{professionalfonts}
%\usefonttheme{serif}

\def\nq{\hspace{-1em}}
\def\look{\(\uparrow\)}
\def\ignore#1{}
\def\deltabar{{\delta\!\!\!^{-}}}
\def\qed{\sqcap\!\!\!\!\sqcup}
\def\odt{{\textstyle{1\over 2}}}
\def\odf{{\textstyle{1\over 4}}}
\def\odA{{\textstyle{1\over A}}}
\def\hbar{h\!\!\!\!^{-}\,}
\def\dbar{d\!\!^{-}\!}
\def\eps{\varepsilon}
\def\beq{\begin{equation}}
\def\eeq{\end{equation}}
\def\beqn{\begin{displaymath}}
\def\eeqn{\end{displaymath}}
\def\bqa{\begin{equation}\begin{array}{c}}
\def\eqa{\end{array}\end{equation}}
\def\bqan{\begin{displaymath}\begin{array}{c}}
\def\eqan{\end{array}\end{displaymath}}
\def\pb{\underline}                       % probability notation
\def\pb#1{\underline{#1}}                 % probability notation
\def\blank{{\,_\sqcup\,}}                 % blank position
\def\maxarg{\mathop{\rm maxarg}}          % maxarg
\def\minarg{\mathop{\rm minarg}}          % minarg
\def\hh#1{{\dot{#1}}}                     % historic I/O
\def\best{*}                              % or {best}
\def\vec#1{{\bf #1}}
\def\length{{l}}

\title{Reinforcement Learning}
\author{Josh Bryan}
\institute{University of Illinois at Chicago, MCS 548}
\date{\today}

\begin{document}

\maketitle

\section{Introduction}
\mode<presentation>{
\subsection{Title}
\begin{frame}
	\titlepage
\end{frame}
\subsection{Contents}
\begin{frame}[allowframebreaks]
	\tableofcontents
\end{frame}
}


\subsection{Reinforcement Learning Overview}

	Reinforcement Learning is an area of AI at the boundary between machine
	learning and decision theory that also incorporates ideas from
	psychology.  The basic learning task is to learn an agent function,
	a function mapping a sequence of observations to an action, by
	actively participating in and exploring an environment.  The
	fundamental idea is that for every action the agent receives some form of
	reward signal.  That signal is the agents indication of success, and
	its task is to maximize the expectation of average or discounted sum
	of rewards.  For the most part this presentation will follow the
	the exposition given in Chapter 13 of \cite{mitchell_machine_1997},
	however ideas will be borrowed from
	\cite{kaelbling_reinforcement_1996} and
	\cite{russell_artificial_2010} as well.  

\begin{frame}
	\frametitle{What is Reinforcement Learning?}
	\begin{block}{}
		Reinforcement learning takes psychology, decision theory, and
		learning theory to answery ``How should an agent learn to act?''
	\end{block}
	\begin{itemize}
		\item Classical Conditioning (see Pavlov's Dog)
		\item Utility Theory
		\item (Partially Observable) Markov Decision Processes
		\item Learns an ``Agent Function''
	\end{itemize}
\end{frame}

\begin{frame}
	\frametitle{Agent Function}
	\begin{itemize}
		\item $O$ is a set of observation symbols.
		\item $A$ is a set of actions.
	\end{itemize}
	\begin{block}{}
		\begin{center}
			$f: O^* \rightarrow A $
		\end{center}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Uses}
	\begin{itemize}
		\item Game playing (e.g. back gammon)
		\item Learning tasks with delayed feedback
		\item Elevator Control
		\item Robotic Control (e.g. stick balancing, car driving)
		\item Simulation Based Approximation Methods (similar to
			ficticious play)
		\item Telecomunication (e.g. learning optimal
			routing)
	\end{itemize}
\end{frame}

\section{Markov Decision Processes}

\subsection{Model Introduction}

A Markov Decision Process describes a system in which an agent exists
in a state $s_t \in S$ at time $t$.  In each state, the agent must choose
an action $a_t \in A$.  After choosing an action, the agent will
transition from $s_t$ to $s_{t+1}$ according to a probability
distribution $T(s_t, a_t, s_{t+1}) = P(s_{t+1} | s_t, a_t)$.

\begin{frame}
	\frametitle{What is an MDP?}
	\begin{block}{Definition: Markov Decision Process}
		An MDP is a tuple $\langle S, A, T, R \rangle$ where:
		\begin{description}
			\item[S] is a set of states.
			\item[A] is a set of actions.
			\item[T] is a transition function. 
				\[ T: S\times A\times S \rightarrow [0,1] \]
			\item[R] is a reward function. 
				\[ R: S\times A \rightarrow \mathbb{R} \] 
		\end{description}
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Simple Example}
	\begin{block}{ As a running example, and as a demo at the end, we
		will be considering this environment:}
		An agent is in a square $10\times 10$ grid world.  Therefore $S =
		\{0,\cdots,9\}^2$
	\end{block}
\end{frame}

\subsection{Policy}

The policy is a mapping from states to actions (or occasionally
probability distributions over actions).  This is a complete
prescription of how the agent should interact in an environment.
Finding an optimal policy will be the learning task.

\begin{frame}
	\frametitle{Policy Defined}
	\begin{block}{Definition: Policy}
		A policy $\pi$ is a mapping from states to actions.
		\[ \pi: S \rightarrow A \]
	\end{block}
\end{frame}

\begin{frame}
	\frametitle{Value of Policy}
\end{frame}

\begin{frame}
	\frametitle{Bellman Equations}
\end{frame}

\subsection{Learning Task}

\begin{frame}
	\frametitle{What are we learning?}
\end{frame}

\begin{frame}
	\frametitle{What are we given?}
\end{frame}

\begin{frame}
	\frametitle{Model Free vs. Model Lased Learning}
\end{frame}

\section{Q-Learning}

\begin{frame}
\end{frame}


\section{SARSA-$\lambda$}


\section{Conclusions}

\subsection{Sources}
\begin{frame}[allowframebreaks]
	\frametitle{Sources}
	\nocite{*}
	\bibliography{reinforcement_learning}
	\bibliographystyle{abbrv}
\end{frame}
\end{document}

